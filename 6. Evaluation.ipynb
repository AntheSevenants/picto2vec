{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc87527f-3c7a-459f-852d-76c59df2f605",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n",
    "\n",
    "How does our system perform matching tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02858f88-cb9e-4f1c-9f63-c2ae6c76fc4c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a0c3c33-ed64-44c8-a1ec-2b2178cf5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "import statistics\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3262dd2e-0796-4aae-b5c8-d28f793ab501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picto2vec.testset import TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894fca04-b414-40e5-990d-3ea0c2608706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89379d3-ae6a-4cb6-8532-5178ced6f15b",
   "metadata": {},
   "source": [
    "## Filtering lexunits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7e16b-a255-4699-a396-05dfca564499",
   "metadata": {},
   "source": [
    "For some reason, I retrieved more senses than we should have. Don't ask. Therefore, I'll make a list of all allowed senses, and we'll see from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5956f79-4d95-4fb3-8be6-763fa5bd0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_lexunits = []\n",
    "with open(\"test_senses.json\", \"rt\") as reader:\n",
    "    senses = json.loads(reader.read())\n",
    "\n",
    "for record in senses:\n",
    "    if record[\"lexunit\"] is None or record[\"lexunit\"] == False:\n",
    "        continue\n",
    "    \n",
    "    legal_lexunits += record[\"lexunit\"].split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118789da-a666-4de9-80b8-1c7338ee7449",
   "metadata": {},
   "source": [
    "For each sense, we get all medoids and associate their representations with their head sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749d7e72-3051-40df-affe-09a667015d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_list(medoid_count, layer_index):\n",
    "    vectors = []\n",
    "    names = []\n",
    "    \n",
    "    layer_path = f\"{VECTORS}/medoid_{medoid_count}/layer_{layer_index}/*\"\n",
    "    sense_file_list = glob(layer_path)\n",
    "    \n",
    "    for sense_file in tqdm(sense_file_list):\n",
    "        if Path(sense_file).stem not in legal_lexunits:\n",
    "            continue\n",
    "        \n",
    "        with open(sense_file, \"rt\") as reader:\n",
    "            sense_vectors = json.loads(reader.read())\n",
    "            if len(sense_vectors) != medoid_count:\n",
    "                raise Exception(f\"Sense vectors should come in groups of {medoid_count}\")\n",
    "                \n",
    "            vectors += sense_vectors\n",
    "                \n",
    "            for i in list(range(0, medoid_count)):\n",
    "                name_tuple = (Path(sense_file).stem, i)\n",
    "                names.append(name_tuple)\n",
    "                \n",
    "    return np.array(vectors), names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad06397a-450c-4b0f-bef2-83d0184ee4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexunit_to_record(lexunit):\n",
    "    for record in senses:\n",
    "        if record[\"lexunit\"] is None or record[\"lexunit\"] == False:\n",
    "            continue\n",
    "            \n",
    "        if lexunit in record[\"lexunit\"]:\n",
    "            return record\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d495edc9-2455-4fc0-b73b-0fefd5d9475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sense_lexunits(lexunit):\n",
    "    return lexunit_to_record(lexunit)[\"lexunit\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec627ba-8c78-43ae-9198-3b530dbb3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexunit_to_synset(lexunit):\n",
    "    return lexunit_to_record(lexunit)[\"synset\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e755772-dd6b-43a7-a005-26d9ce255e09",
   "metadata": {},
   "source": [
    "## Actual evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e00a14-2f9f-43d3-beb8-94285249fcfc",
   "metadata": {},
   "source": [
    "We define our medoids again, and the number of layers of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c779b70f-10f2-4ca5-b80e-ede39ea29a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "medoids = [3, 5, 7, 10]\n",
    "layer_indices = list(range(0, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5930a8-888f-42ad-b55d-da3b65d39dd1",
   "metadata": {},
   "source": [
    "We will test the system's performance using all possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72f9fbe0-7cbb-446a-ba25-40723926fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = list(itertools.product(medoids, layer_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0907c-3155-49b6-bc38-abbbdbf7b475",
   "metadata": {},
   "source": [
    "Evaluation is done for a medoid count and a layer index. The evaluation function will load all test set representations (which are conveniently also already created, they are all representations beyond :30 in our dataset). We let the system predict the correct lexunit and compute some evaluation measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ceca5038-3fcc-4877-8349-985738e809ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(medoid_count, layer_index):\n",
    "    candidates, names = create_vector_list(medoid_count, layer_index)\n",
    "    candidate_tree = cKDTree(candidates)\n",
    "    \n",
    "    sense_files = glob(f\"{SENSE_EX_VEC}/*\")\n",
    "    sense_files = list(filter(lambda sense_file: Path(sense_file).stem in legal_lexunits, sense_files))\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    labels = []\n",
    "    certainty = []\n",
    "    \n",
    "    for sense_file in tqdm(sense_files):\n",
    "        with open(sense_file, \"rt\") as reader:\n",
    "            data = json.loads(reader.read())\n",
    "    \n",
    "        sense = TestSet(Path(sense_file).stem, data)\n",
    "        \n",
    "        #if sense.total_representation_count() <= 30:\n",
    "        #    continue\n",
    "            \n",
    "        if sense.name not in legal_lexunits:\n",
    "            continue\n",
    "        \n",
    "        synset = lexunit_to_synset(sense.name)\n",
    "                    \n",
    "        representations = sense.get_representations(layer_index)\n",
    "        for representation in representations:\n",
    "            # Add correct prediction and label to list\n",
    "            y_true.append(synset)\n",
    "            labels.append(synset)\n",
    "            \n",
    "            token_vector = np.array(representation)\n",
    "\n",
    "            nearest_neighbor_indices = candidate_tree.query(token_vector, k=medoid_count)[1]\n",
    "            results_lexunit_tuples = list(map(lambda neighbour_index: names[neighbour_index], nearest_neighbor_indices))\n",
    "            results = list(map(lambda lexunit_tuple: (lexunit_to_synset(lexunit_tuple[0]), lexunit_tuple[1]), results_lexunit_tuples))\n",
    "            \n",
    "            sense_guesses = {}\n",
    "            for sense_medoid, medoid_index in results:\n",
    "                if not sense_medoid in sense_guesses:\n",
    "                    sense_guesses[sense_medoid] = 0\n",
    "                \n",
    "                sense_guesses[sense_medoid] += 1\n",
    "        \n",
    "            # TODO: what if we have a tie?\n",
    "            max_value = max(sense_guesses.values())\n",
    "            guesses = [key for key, value in sense_guesses.items() if value == max_value]\n",
    "            #print(guesses)\n",
    "            \n",
    "            if len(guesses) == 1:\n",
    "                y_pred.append(guesses[0])\n",
    "                certainty.append(max_value / medoid_count)\n",
    "            else:\n",
    "                y_pred.append(\"UNSURE\")\n",
    "                certainty.append(None)\n",
    "                \n",
    "    return y_true, y_pred, labels, certainty, (y_pred.count(\"UNSURE\") / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1955dc7-f55d-4ede-887c-d8a41c5cea00",
   "metadata": {},
   "source": [
    "We tie everything together using multi processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88b80b68-38a5-4c33-90bb-4310d9045d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "results = pool.starmap(evaluate, combinations)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860c483-cfaf-4a56-8820-138f058d3a23",
   "metadata": {},
   "source": [
    "We write all res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e50c2979-e617-446c-b92a-728c721b1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"wt\") as writer:\n",
    "    writer.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78b03d11-4b68-4e5b-8c70-64f89b71abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "index = 0\n",
    "for y_true, y_pred, labels, certainty, unsure_ratio in results:\n",
    "    medoid_count, layer_index = combinations[index]\n",
    "    \n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred, labels=labels, average=\"macro\")\n",
    "    filtered = list(filter(lambda entry: entry is not None, certainty))\n",
    "    mean_certainty = 0\n",
    "    if len(filtered) > 0:\n",
    "        mean_certainty = statistics.mean(filtered)\n",
    "    \n",
    "    row = { \"medoid_count\": medoid_count,\n",
    "            \"layer_index\": layer_index,\n",
    "            \"f1\": f1,\n",
    "            \"mean_certainty\": mean_certainty,\n",
    "            \"unsure_ratio\": unsure_ratio }\n",
    "    df_rows.append(row)\n",
    "    index += 1\n",
    "df = pd.DataFrame.from_dict(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e4a01b9-045c-4669-abe7-5ef6c460d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "91677392-691c-4f14-9623-a8307b587897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b041c409bd264be7bdf6626c264fa527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10230 10230\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388310e684e94caebeabf7b74630d338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true, y_pred, labels, certainty, unsure_ratio = evaluate(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "288b4093-3da3-4e7c-8d3f-cc6eaee3c0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7062645184960138"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_true, y_pred, labels=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f88e2b35-1a57-4680-9842-b378087babe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658324059189646"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(list(filter(lambda entry: entry is not None, certainty)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9d5289a3-dbe0-4b35-ba26-d96e7f512451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18769879800448655"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsure_ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
