{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e23f0dc-5e06-44cb-8e00-f3611bece00d",
   "metadata": {},
   "source": [
    "# 3. Get corpus examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c069a-07bc-4697-ab44-3ce9bbb00741",
   "metadata": {},
   "source": [
    "Now that we finally know which corpus examples can be found, we will look for those corpus examples and collect them in files, aggregated by lexunit id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11bb097-5d0b-447c-86e5-91d8b47f0d78",
   "metadata": {},
   "source": [
    "To read COREX files, we use my [plk2xml](https://github.com/AntheSevenants/plk2xml) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d16bf44-c532-49af-833f-7b3e2542f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import lxml.etree as etree\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from plk2xml.plk2xml.plk import plk\n",
    "from plk2xml.plk2xml.plk2xml import plk2xml\n",
    "from plk2xml.plk2xml.plk2json import plk2json\n",
    "from glob import glob\n",
    "import gc\n",
    "NO_SAMPLES = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aaa7d45-dc87-4892-9933-4321eb087724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14407706-a74a-4e35-a064-bbcbf2ebb0a7",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35608873-167c-46ef-a130-714b45cbc839",
   "metadata": {},
   "source": [
    "First, let's load in the dataset we created in the previous notebook (the JSONL files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f406994-27f9-4185-a57c-ac187903ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_files = glob(f\"sense_example_references/*.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5d57b-0a52-4302-9abf-60c5ef77f525",
   "metadata": {},
   "source": [
    "## Functions for retrieving corpus examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41978b4f-92da-4a49-a3e2-a547a096d752",
   "metadata": {},
   "source": [
    "This function retrieves all words from a sentence XML element, either for FOLIA or PLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64aca67-10ae-4968-8adf-80b0e521e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_to_sentence(element, element_type=\"folia\"):\n",
    "    if element_type == \"folia\":\n",
    "        t_elements = element.xpath(\"./folia:w/folia:t\", namespaces=ns)\n",
    "    elif element_type == \"corex\":\n",
    "        t_elements = element.xpath(\"./token/form\")\n",
    "        \n",
    "    words = list(map(lambda element: element.text, t_elements))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641dc37-d7a5-46e0-b57e-8d22421ffde4",
   "metadata": {},
   "source": [
    "Processing of a FOLIA file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b1928c-4013-4402-ace1-323026f690e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = { \"folia\": \"http://ilk.uvt.nl/folia\",\n",
    "       \"xml\": \"http://www.w3.org/XML/1998/namespace\" }\n",
    "\n",
    "def folia_to_sentences(xml_filename, xml_id):\n",
    "    if not xml_filename in parsed_file_cache:\n",
    "        root = etree.parse(xml_filename)\n",
    "        #parsed_file_cache[xml_filename] = root\n",
    "    else:\n",
    "        root = parsed_file_cache[xml_filename]\n",
    "\n",
    "    xmlid_regex = re.search(r\"(.*?s\\.)(\\d+)\\.w\\.(\\d+)$\", xml_id).groups()\n",
    "    sentence_id_prefix = xmlid_regex[0]\n",
    "    sentence_id = int(xmlid_regex[1])\n",
    "    word_index = int(xmlid_regex[2])\n",
    "    \n",
    "    center_sentence_xpath = f\"//folia:s[@xml:id='{row['docid']}.{sentence_id_prefix}{sentence_id}']\"\n",
    "    left_sentence_xpath = f\"//folia:s[@xml:id='{row['docid']}.{sentence_id_prefix}{sentence_id - 1}']\"\n",
    "    right_sentence_xpath = f\"//folia:s[@xml:id='{row['docid']}.{sentence_id_prefix}{sentence_id + 1}']\"\n",
    "    \n",
    "    center_sentence_xpath = root.xpath(center_sentence_xpath, namespaces=ns)\n",
    "    left_sentence_xpath = root.xpath(left_sentence_xpath, namespaces=ns)\n",
    "    right_sentence_xpath = root.xpath(right_sentence_xpath, namespaces=ns)\n",
    "    \n",
    "    return center_sentence_xpath, left_sentence_xpath, right_sentence_xpath, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc397ef-41bc-4f7a-ad63-07d3cdf8797c",
   "metadata": {},
   "source": [
    "Processing of a PLK file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d356eecc-e70c-488d-8c35-35d7169c0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plk_to_sentences(plk_filename, xml_id):\n",
    "    if not plk_filename in parsed_file_cache:\n",
    "        plk_file = plk(plk_filename).data\n",
    "        #parsed_file_cache[plk_filename] = plk_file\n",
    "    else:\n",
    "        plk_file = parsed_file_cache[plk_filename]\n",
    "        \n",
    "    sentence_info_regex = re.search(r\"^s\\.(\\d+)\\.w\\.(\\d+)\", xml_id).groups()\n",
    "    sentence_id = int(sentence_info_regex[0]) - 1 # Python is 0-indexed\n",
    "    word_index = int(sentence_info_regex[1])\n",
    "    \n",
    "    center_sentence = [token[\"form\"] for token in plk_file[sentence_id][\"tokens\"]]\n",
    "    \n",
    "    left_sentence = None\n",
    "    if sentence_id - 1 >= 0:\n",
    "        left_sentence = [token[\"form\"] for token in plk_file[sentence_id - 1][\"tokens\"]]\n",
    "        \n",
    "    right_sentence = None\n",
    "    if sentence_id + 1 < len(plk_file):\n",
    "        right_sentence = [token[\"form\"] for token in plk_file[sentence_id + 1][\"tokens\"]]\n",
    "    \n",
    "    return center_sentence, left_sentence, right_sentence, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bbfeb-045a-46de-99c5-ea140d9fd723",
   "metadata": {},
   "source": [
    "## Retrieving all examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4038eb-f356-4022-8a27-2d4f275c1453",
   "metadata": {},
   "source": [
    "Now, we will go over all JSONL files and read each line. We throw away all \"lost\" references. Then, we sample the required number of corpus examples and retrieve these examples. The exact method of retrieval is different for each source. Corpus examples are aggregated per sense, and saved as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a2730-db4e-4abd-bfdb-96e4c645a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We go over all files with unique senses we have\n",
    "for jsonl_file in tqdm(jsonl_files):\n",
    "    parsed_file_cache = {}\n",
    "    data = []\n",
    "    candidates = []\n",
    "    with open(jsonl_file, \"rt\") as reader:\n",
    "        # We loop overall references\n",
    "        for json_line in reader:\n",
    "            row = json.loads(json_line)\n",
    "            \n",
    "            # Do not process lost tokens\n",
    "            if row[\"lost\"]:\n",
    "                continue\n",
    "                \n",
    "            candidates.append(row)\n",
    "            \n",
    "        if len(candidates) < NO_SAMPLES:\n",
    "            pass\n",
    "        else:\n",
    "            random.seed(1337)\n",
    "            candidates = random.sample(candidates, NO_SAMPLES)\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            print(f\"{jsonl_file} has no candidates!\")\n",
    "        \n",
    "        # We check where the reference is coming from\n",
    "        for row in tqdm(candidates, leave=False):\n",
    "            # CGN\n",
    "            if row[\"docid\"].startswith(\"CGN\"):\n",
    "                cgn_id = re.search(r\"_(.*?)$\", row[\"docid\"]).groups()[0]\n",
    "                plk_filename = f\"{SONAR_PATH}{cgn_id}.plk\"\n",
    "                    \n",
    "                center_sentence, left_sentence, right_sentence, word_index = plk_to_sentences(plk_filename, row[\"xmlid\"])\n",
    "                \n",
    "                element_type = None\n",
    "            # Lassy (pre-processed)\n",
    "            elif row[\"docid\"].startswith(\"WR-P-P-I\"):\n",
    "               \n",
    "                lassy_hit = lassy_hits.loc[(lassy_hits[\"docid\"] == row[\"docid\"]) & \n",
    "                                       (lassy_hits[\"xmlid\"] == row[\"xmlid\"])]               \n",
    "                lassy_hit = lassy_hit[0]\n",
    "                \n",
    "                center_sentence = lassy_hit[\"center_sentence\"]\n",
    "                left_sentence = lassy_hit[\"left_sentence\"]\n",
    "                right_sentence = lassy_hit[\"right_sentence\"]\n",
    "                \n",
    "                word_index = None\n",
    "                \n",
    "                element_type = None\n",
    "            # SONAR\n",
    "            else:\n",
    "                xml_filename = f\"{SONAR_PATH}{row['docid']}.folia.xml\"\n",
    "            \n",
    "                center_sentence_xpath, left_sentence_xpath, right_sentence_xpath, word_index = folia_to_sentences(xml_filename, row[\"xmlid\"])\n",
    "                element_type = \"folia\"\n",
    "            \n",
    "            if element_type is not None:\n",
    "                center_sentence = element_to_sentence(center_sentence_xpath[0], element_type) if len(center_sentence_xpath) > 0 else None\n",
    "                left_sentence = element_to_sentence(left_sentence_xpath[0], element_type) if len(left_sentence_xpath) > 0 else None\n",
    "                right_sentence = element_to_sentence(right_sentence_xpath[0], element_type) if len(right_sentence_xpath) > 0 else None\n",
    "        \n",
    "            data_entry = { \"docid\": row[\"docid\"],\n",
    "                           \"lemma\": row[\"lemma\"],\n",
    "                           \"pos\": row[\"pos\"],\n",
    "                           \"sense\": row[\"sense\"],\n",
    "                           \"xmlid\": row[\"xmlid\"],\n",
    "                           \"word_index\": word_index,\n",
    "                           \"center_sentence\": center_sentence,\n",
    "                           \"left_sentence\": left_sentence,\n",
    "                           \"right_sentence\": right_sentence }\n",
    "            \n",
    "            data.append(data_entry)\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df.to_csv(f\"{SENSE_EX}{row['sense']}.csv\", index=False)\n",
    "        \n",
    "        del parsed_file_cache\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
