{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3360209-e747-48b6-835e-0df264ee1bfc",
   "metadata": {},
   "source": [
    "# 4. Get sense vectors\n",
    "\n",
    "The code in this notebook is aimed at creating reference vector representations for each sense in the dataset with WordNet senses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346f791-431e-434e-8492-d1a3eacc6344",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81890d2b-9479-4506-897b-47a2e7c83566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abd183-3ca1-462e-b082-f6f114d9cdca",
   "metadata": {},
   "source": [
    "We use [anthevec](https://github.com/AntheSevenants/anthevec) to get vectors for whole words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b3be7c-5f29-4573-b4d2-38609c9fa2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthevec.anthevec.embedding_retriever import EmbeddingRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e64bc80-27be-4c1f-849d-7f7dbedbc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682eced-b6c8-4605-92d4-671df50560e2",
   "metadata": {},
   "source": [
    "## Load senses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357de871-1928-46de-bdae-a1de6205d4ae",
   "metadata": {},
   "source": [
    "We will create representations for all corpus examples found for a specific sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9250754f-7bec-4d8f-b05f-120e4f1d0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sense_example_filenames = glob(f\"{SENSE_EX}*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6797801-72f0-4101-ba02-c6540716ed09",
   "metadata": {},
   "source": [
    "## Transformer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8033aa88-60e4-4ea3-a61c-c33fd0f74556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pdelobelle/robbert-v2-dutch-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, RobertaModel, RobertaConfig\n",
    "\n",
    "MODEL_NAME = \"pdelobelle/robbert-v2-dutch-base\"\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "config = RobertaConfig.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35a5271-d6aa-4423-bd85-8978cc4910d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# We need to load spacy, since the BERT tokeniser will split into word pieces,\n",
    "# and not individual words. We're interested in words, so we get an \"alternative\"\n",
    "# tokeniser, which we'll map the word pieces on.\n",
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8d6d8-cc53-479f-89d3-ba3ab4793d7a",
   "metadata": {},
   "source": [
    "## Creating representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5848cb-9883-4215-840d-047438ddcaef",
   "metadata": {},
   "source": [
    "Now, we find all senses in our dataset and create vectors for each corpus example of each sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c64237-88eb-4f67-86b9-06ae4b828d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sense_example_filename in tqdm(sense_example_filenames):\n",
    "    sense_vectors = {} # will hold all vectors for this sense\n",
    "    \n",
    "    # Find all example sentences for this sense\n",
    "    try:\n",
    "        corpus_examples = pd.read_csv(sense_example_filename)\n",
    "    except:\n",
    "        # No file contents\n",
    "        continue\n",
    "        \n",
    "    output_path = f\"{SENSE_EX_VEC}/{corpus_examples.iloc[0]['sense']}.json\"\n",
    "    if os.path.exists(output_path):\n",
    "        continue\n",
    "    \n",
    "    # For each example sentence, compute a vector\n",
    "    for index, row in corpus_examples.iterrows():\n",
    "        key = row[\"docid\"] + \"_\" + row[\"xmlid\"]\n",
    "        \n",
    "        try:\n",
    "            embedding_retriever = EmbeddingRetriever(model, tokenizer, nlp, [ row[\"center_sentence\"].split(\" \") ])\n",
    "            \n",
    "            # Word indices of lassy tokens are unreliable\n",
    "            # We find them ourselves\n",
    "            if row[\"word_index\"] is None:\n",
    "                lemmas = list(map(lambda token: token.lemma_, embedding_retriever.tokens[sentence_index]))\n",
    "                word_index = lemmas.index(self.lemma)\n",
    "            else:\n",
    "                word_index = row[\"word_index\"] - 1\n",
    "\n",
    "            # We create vectors for each layer separately...\n",
    "            vectors = []\n",
    "            for layer_index in list(range(1, 13)):\n",
    "                vector = embedding_retriever.get_hidden_state(0, word_index, [ layer_index ])\n",
    "                vector = list(vector.astype('float64'))\n",
    "                vectors.append(vector)\n",
    "                \n",
    "            # ...and also append the layer average \n",
    "            vectors.append(list(embedding_retriever.get_hidden_state(0, word_index, list(range(1, 13))).astype('float64')))\n",
    "                \n",
    "            sense_vectors[key] = vectors\n",
    "        # Sometimes there will be errors, because this methodology isn't perfect. But it won't matter in the end because we have enough representations.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(word_index, row[\"xmlid\"], row[\"center_sentence\"])\n",
    "            sense_vectors[key] = None\n",
    "    \n",
    "    with open(output_path, \"wt\") as writer:\n",
    "        writer.write(json.dumps(sense_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811976d-6e88-4a37-af42-c21aaf7a8df8",
   "metadata": {},
   "source": [
    "Each sense gets its own JSON file. Inside which the first twelve elements represent layers 1 to 12, and the final element represents the average."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
